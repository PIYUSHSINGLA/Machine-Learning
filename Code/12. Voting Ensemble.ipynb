{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___Voting Ensemble - Supervised Machine Learning___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___What is Voting Ensemble?___\n",
    "\n",
    "_A voting ensemble (or a “majority voting ensemble“) is an ensemble machine learning model that combines the predictions from multiple other models._\n",
    "\n",
    "_It is a technique that may be used to improve model performance, ideally achieving better performance than any single model used in the ensemble._\n",
    "\n",
    "_A voting ensemble works by combining the predictions from multiple models. It can be used for classification or regression. In the case of regression, this involves calculating the average of the predictions from the models. In the case of classification, the predictions for each label are summed and the label with the majority vote is predicted._\n",
    "\n",
    "_There are two approaches to the majority vote prediction for classification; they are hard voting and soft voting._\n",
    "\n",
    "_Hard voting involves summing the predictions for each class label and predicting the class label with the most votes. Soft voting involves summing the predicted probabilities (or probability-like scores) for each class label and predicting the class label with the largest probability._\n",
    "\n",
    "### ___Hard Voting___\n",
    "_In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction._\n",
    "\n",
    "### ___Soft Voting___\n",
    "_In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier._\n",
    "\n",
    "_A voting ensemble may be considered a meta-model, a model of models._\n",
    "\n",
    "_As a meta-model, it could be used with any collection of existing trained machine learning models and the existing models do not need to be aware that they are being used in the ensemble. This means you could explore using a voting ensemble on any set or subset of fit models for your predictive modeling task._\n",
    "\n",
    "_A voting ensemble is appropriate when you have two or more models that perform well on a predictive modeling task. The models used in the ensemble must mostly agree with their predictions._\n",
    "\n",
    "___Use voting ensembles when:___\n",
    "\n",
    "* _All models in the ensemble have generally the same good performance._\n",
    "* _All models in the ensemble mostly already agree._\n",
    "\n",
    "___Hard voting___ _is appropriate when the models used in the voting ensemble predict crisp class labels. Soft voting is appropriate when the models used in the voting ensemble predict the probability of class membership._\n",
    "\n",
    "___Soft voting___ _can be used for models that do not natively predict a class membership probability, although may require calibration of their probability-like scores prior to being used in the ensemble (e.g. support vector machine, k-nearest neighbors, and decision trees)._\n",
    "\n",
    "* _Hard voting is for models that predict class labels._\n",
    "* _Soft voting is for models that predict class membership probabilities._\n",
    "\n",
    "_The voting ensemble is not guaranteed to provide better performance than any single model used in the ensemble. If any given model used in the ensemble performs better than the voting ensemble, that model should probably be used instead of the voting ensemble._\n",
    "\n",
    "_This is not always the case. A voting ensemble can offer lower variance in the predictions made over individual models. This can be seen in a lower variance in prediction error for regression tasks. This can also be seen in a lower variance in accuracy for classification tasks. This lower variance may result in a lower mean performance of the ensemble, which might be desirable given the higher stability or confidence of the model._\n",
    "\n",
    "___Use a voting ensemble if:___\n",
    "\n",
    "* _It results in better performance than any model used in the ensemble._\n",
    "* _It results in a lower variance than any model used in the ensemble._\n",
    "\n",
    "_A voting ensemble is particularly useful for machine learning models that use a stochastic learning algorithm and result in a different final model each time it is trained on the same dataset._ \n",
    "\n",
    "_Another particularly useful case for voting ensembles is when combining multiple fits of the same machine learning algorithm with slightly different hyperparameters._\n",
    "\n",
    "___Voting ensembles are most effective when:___\n",
    "\n",
    "* _Combining multiple fits of a model trained using stochastic learning algorithms._\n",
    "* _Combining multiple fits of a model with different hyperparameters._\n",
    "\n",
    "_A limitation of the voting ensemble is that it treats all models the same, meaning all models contribute equally to the prediction. This is a problem if some models are good in some situations and poor in others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "# loading iris dataset \n",
    "iris = load_iris() \n",
    "X = iris.data[:, :4] \n",
    "Y = iris.target \n",
    "  \n",
    "# train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.20,random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group / ensemble of models \n",
    "estimator = [] \n",
    "estimator.append(('LR',  \n",
    "                  LogisticRegression(solver ='lbfgs',  \n",
    "                                     multi_class ='multinomial',  \n",
    "                                     max_iter = 200))) \n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True))) \n",
    "estimator.append(('DTC', DecisionTreeClassifier())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Score  1\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier with hard voting \n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard') \n",
    "vot_hard.fit(X_train, y_train) \n",
    "y_pred = vot_hard.predict(X_test) \n",
    "  \n",
    "# using accuracy_score metric to predict accuracy \n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Hard Voting Score % d\" % score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Score  1\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier with soft voting \n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "vot_soft.fit(X_train, y_train) \n",
    "y_pred = vot_soft.predict(X_test) \n",
    "  \n",
    "# using accuracy_score \n",
    "score = accuracy_score(y_test, y_pred) \n",
    "print(\"Soft Voting Score % d\" % score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We can further tune the model with Hyperparamtere tuning and get best parameters for the models taken to train the final model._\n",
    "\n",
    "_In practical the output accuracy will be more for soft voting as it is the average probability of the all estimators combined, as for our basic iris dataset we are already overfitting, so there won’t be much difference in output._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
